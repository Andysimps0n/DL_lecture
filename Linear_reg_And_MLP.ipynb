{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Pytorch (For Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch==1.0.0 torchvision\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23sQEXhSfY4I"
   },
   "source": [
    "### 1. Data Generation  (Preparation)\n",
    "\n",
    "Regression 실습에 사용할 데이터를 만들어봅시다. 이번에는 X가 2차원 Y가 1차원인 상황입니다.  \n",
    "아래 관계를 따르는 데이터 분포가 있으며 관측시 발생한 오차가 $e$가 더해져 있다고 합시다.  \n",
    "$sin(x)$ 함수와 $log(x)$ 함수 모두 쉽게 생각할 수 있는 non-linear 함수이죠!  \n",
    "\n",
    "$$ e \\sim \\mathcal{N} (0, 0.5) $$\n",
    "$$y = \\ 2 sin(x_1) + log({1 \\over 2}x_2^2) + e$$  \n",
    "\n",
    "\n",
    "**Data Set**  \n",
    "$$X_{train} \\in \\mathcal{R}^{1600 \\times 2}, Y_{train} \\in \\mathcal{R}^{1600}$$  \n",
    "$$X_{val} \\in \\mathcal{R}^{400 \\times 2}, Y_{val} \\in \\mathcal{R}^{400}$$  \n",
    "$$X_{test} \\in \\mathcal{R}^{400 \\times 2}, Y_{test} \\in \\mathcal{R}^{400}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vUTWMXilI0-"
   },
   "source": [
    "X, Y 데이터를 3D 공간에 시각화 해볼까요?  \n",
    "`numpy`의 `random.rand()` 함수를 사용하면 n개의 랜덤 샘플을 0~1의 uniform distribution에서 샘플링할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "rvmGZhJ-HHcB",
    "outputId": "2d3e79e9-d3a5-4518-b91b-357479066570"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "# ====== Generating Dataset ====== #\n",
    "num_data = 2400\n",
    "x1 = np.random.rand(num_data) * 10\n",
    "x2 = np.random.rand(num_data) * 10\n",
    "e = np.random.normal(0, 0.5, num_data)\n",
    "X = np.array([x1, x2]).T\n",
    "y = 2*np.sin(x1) + np.log(0.5*x2**2) + e\n",
    "# ====== Split Dataset into Train, Validation, Test ======#\n",
    "train_X, train_y = X[:1600, :], y[:1600]\n",
    "val_X, val_y = X[1600:2000, :], y[1600:2000]\n",
    "test_X, test_y = X[2000:, :], y[2000:]\n",
    "\n",
    "# ====== Visualize Each Dataset ====== #\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "ax1.scatter(train_X[:, 0], train_X[:, 1], train_y, c=train_y, cmap='jet')\n",
    "\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax1.set_zlabel('y')\n",
    "ax1.set_title('Train Set Distribution')\n",
    "ax1.set_zlim(-10, 6)\n",
    "ax1.view_init(40, -60)\n",
    "ax1.invert_xaxis()\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "ax2.scatter(val_X[:, 0], val_X[:, 1], val_y, c=val_y, cmap='jet')\n",
    "\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "ax2.set_zlabel('y')\n",
    "ax2.set_title('Validation Set Distribution')\n",
    "ax2.set_zlim(-10, 6)\n",
    "ax2.view_init(40, -60)\n",
    "ax2.invert_xaxis()\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "ax3.scatter(test_X[:, 0], test_X[:, 1], test_y, c=test_y, cmap='jet')\n",
    "\n",
    "ax3.set_xlabel('x1')\n",
    "ax3.set_ylabel('x2')\n",
    "ax3.set_zlabel('y')\n",
    "ax3.set_title('Test Set Distribution')\n",
    "ax3.set_zlim(-10, 6)\n",
    "ax3.view_init(40, -60)\n",
    "ax3.invert_xaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEqtbTROlXuB"
   },
   "source": [
    "### 2. Hypothesis Define (Model Define)  \n",
    "\n",
    "Linear Regression 시에 활용하는 H(x) = Wx + b를 파이토치로 구현할 때는 단순하게 `nn.Linear` 모듈을 하나 만들어주면 됩니다. 이 때`in_features`는 x의 차원이고, `out_features`는 y의 차원입니다.  \n",
    "\n",
    "\n",
    "**Linear Model**   \n",
    "$$H = \\ XW + b \\ \\ ( W \\in \\mathcal{R}^{2 \\times 1}, b \\in \\mathcal{R}^{1}, H \\in \\mathcal{R}^{N \\times 1})$$\n",
    "\n",
    "\n",
    "**MLP Model**\n",
    "$$Let \\ relu(X) = \\ max(X, 0)$$  \n",
    "\n",
    "$$h = \\ relu(X W_1 + b_1) \\ \\  ( W_1 \\in \\mathcal{R}^{2 \\times 200}, b_1 \\in \\mathcal{R}^{200}, h \\in \\mathcal{R}^{N \\times 200})$$  \n",
    "\n",
    "$$H = \\ h W_2 + b_2  \\ \\  ( W_2 \\in \\mathcal{R}^{200 \\times 1}, b_2 \\in \\mathcal{R}^{1}, H \\in \\mathcal{R}^{N  \\times 1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "40ht43Q1JpYI",
    "outputId": "2e740bff-a553-4f68-8091-23c77fc6d5be"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "    # 인스턴스(샘플) x가 인풋으로 들어왔을 때 모델이 예측하는 y값을 리턴합니다.\n",
    "        return self.linear(x)\n",
    "    \n",
    "    \n",
    "class MLPModel(nn.Module):\n",
    "  def __init__(self, layer_num):\n",
    "    super(MLPModel, self).__init__()\n",
    "\n",
    "    self.layers = nn.ModuleList([\n",
    "        nn.Linear(2**i, 2**(i+1)) for i in range(1, layer_num+1)\n",
    "    ])\n",
    "\n",
    "    self.activation = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear1(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.linear2(x)\n",
    "    x = self.activation(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FgdSvskSotgI"
   },
   "source": [
    "### 3. Cost Function Define (Loss Function Define)  \n",
    "\n",
    "Regression Problem 에서의 Loss Function을 작성해봅시다.  \n",
    "파이토치의 `nn` 아래에는 다양한 Loss Function이 이미 구현되어 있습니다.  \n",
    "[여기](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html)에서 파이토치 내부에 구현되어 있는 MSE, Cross-Entropy, KL-Divergence 등을 확인할 수 있습니다.  \n",
    "이 실습에서는 Regression Problem 이므로 Mean Squared Error 즉 MSE Loss를 써봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "yzNkZ_uwnufC",
    "outputId": "d6edd719-7915-4931-e16a-d6d618578da0"
   },
   "outputs": [],
   "source": [
    "reg_loss = nn.MSELoss()\n",
    "\n",
    "\"\"\"uncomment for testing Loss Function\n",
    "test_pred_y = torch.Tensor([0,0,0,0])\n",
    "test_true_y = torch.Tensor([0,1,0,1])\n",
    "\n",
    "print(reg_loss(test_pred_y, test_true_y))\n",
    "print(reg_loss(test_true_y, test_true_y))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train & Evaluation\n",
    "이제 모델도 정의했고 Loss Function도 정의했으니 한번 학습시켜볼까요?  \n",
    "저번에는 저희가 직접 파라미터(w 하나)에 따른 loss의 편미분을 유도하고 거기서 나온 그라디언트로 w를 업데이트했는데요.  \n",
    "`pytorch`에서는 loss.backward() 라는 기능을 쓰면 알아서 loss를 계산되는데 사용한 각 파라미터에 대한 loss의 편미분을 계산해줍니다.  \n",
    "이후 `optimizer.step()` 함수를 사용하면 각 파라미터의 그라디언트를 바탕으로 파라미터의 값을 조금씩 업데이트 해줍니다. \n",
    "\n",
    "이 때 모델은 매 Iteration 때마다 Train Set에 의해 학습되면서 동시에 Validation Set을 넣고 Loss를 비교하게됩니다. \n",
    "그리고 저희가 평가하고 싶은 때마다 (지금은 200번) Test Set을 넣고 Visualize 하는데 필요한 Predicted y값과 Metric 값을 계산합니다. 여기서 Metric으로는 mean absolute error(MAE)를 활용해보죵  \n",
    "\n",
    "$$MAE(Y_{true}, Y_{predict}) = \\sum_{i} | \\ y_{true}^{(i)} - y_{predict}^{(i)} \\ | $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# ====== Construct Model ====== #\n",
    "# model = LinearModel()\n",
    "# print(model.linear.weight)\n",
    "# print(model.linear.bias)\n",
    "\n",
    "model = MLPModel() # Model을 생성해줍니다.\n",
    "print('{} parameters'.format(sum(p.numel() for p in model.parameters() if p.requires_grad))) # 복잡해보이지만 간단히 모델 내에 학습을 당할 파라미터 수를 카운팅하는 코드입니다.\n",
    "\n",
    "# ===== Construct Optimizer ====== #\n",
    "lr = 0.005 # Learning Rate를 하나 정해줍니다. (원할한 학습을 위해 손을 많이 탑니다)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr) # Optimizer를 생성해줍니다.\n",
    "\n",
    "# 매 학습 단계에서의 epoch값과 그 때의 loss 값을 저장할 리스트를 만들어줍시다.\n",
    "list_epoch = [] \n",
    "list_train_loss = []\n",
    "list_val_loss = []\n",
    "list_mae = []\n",
    "list_mae_epoch = []\n",
    "\n",
    "\n",
    "epoch = 4000 # 학습 횟수(epoch)을 지정해줍시다.\n",
    "for i in range(epoch):\n",
    "    \n",
    "    # ====== Train ====== #\n",
    "    model.train() # model을 train 모드로 세팅합니다. 반대로 향후 모델을 평가할 때는 eval() 모드로 변경할 겁니다 (나중 실습에서 쓸 겁니다)\n",
    "    optimizer.zero_grad() # optimizer에 남아있을 수도 있는 잔여 그라디언트를 0으로 다 초기화해줍니다.\n",
    "    \n",
    "    input_x = torch.Tensor(train_X)\n",
    "    true_y = torch.Tensor(train_y)\n",
    "    pred_y = model(input_x)\n",
    "    #print(input_x.shape, true_y.shape, pred_y.shape) # 각 인풋과 아웃풋의 차원을 체크해봅니다.\n",
    "    \n",
    "    loss = reg_loss(pred_y.squeeze(), true_y)\n",
    "    loss.backward() # backward()를 통해서 그라디언트를 구해줍니다.\n",
    "    optimizer.step() # step()을 통해서 그라디언틀르 바탕으로 파라미터를 업데이트 해줍니다. \n",
    "    list_epoch.append(i)\n",
    "    list_train_loss.append(loss.detach().numpy())\n",
    "    \n",
    "    \n",
    "    # ====== Validation ====== #\n",
    "    model.eval()\n",
    "    optimizer.zero_grad()\n",
    "    input_x = torch.Tensor(val_X)\n",
    "    true_y = torch.Tensor(val_y)\n",
    "    pred_y = model(input_x)   \n",
    "    loss = reg_loss(pred_y.squeeze(), true_y)\n",
    "    list_val_loss.append(loss.detach().numpy())\n",
    "    \n",
    "\n",
    "    # ====== Evaluation ======= #\n",
    "    if i % 200 == 0: # 200회의 학습마다 실제 데이터 분포와 모델이 예측한 분포를 그려봅니다.\n",
    "        \n",
    "        # ====== Calculate MAE ====== #\n",
    "        model.eval()\n",
    "        optimizer.zero_grad()\n",
    "        input_x = torch.Tensor(test_X)\n",
    "        true_y = torch.Tensor(test_y)\n",
    "        pred_y = model(input_x).detach().numpy() \n",
    "        mae = mean_absolute_error(true_y, pred_y) # sklearn 쪽 함수들은 true_y 가 먼저, pred_y가 나중에 인자로 들어가는 것에 주의합시다\n",
    "        list_mae.append(mae)\n",
    "        list_mae_epoch.append(i)\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,5))\n",
    "        \n",
    "        # ====== True Y Scattering ====== #\n",
    "        ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "        ax1.scatter(test_X[:, 0], test_X[:, 1], test_y, c=test_y, cmap='jet')\n",
    "        \n",
    "        ax1.set_xlabel('x1')\n",
    "        ax1.set_ylabel('x2')\n",
    "        ax1.set_zlabel('y')\n",
    "        ax1.set_zlim(-10, 6)\n",
    "        ax1.view_init(40, -40)\n",
    "        ax1.set_title('True test y')\n",
    "        ax1.invert_xaxis()\n",
    "\n",
    "        # ====== Predicted Y Scattering ====== #\n",
    "        ax2 = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "        ax2.scatter(test_X[:, 0], test_X[:, 1], pred_y, c=pred_y[:,0], cmap='jet')\n",
    "\n",
    "        ax2.set_xlabel('x1')\n",
    "        ax2.set_ylabel('x2')\n",
    "        ax2.set_zlabel('y')\n",
    "        ax2.set_zlim(-10, 6)\n",
    "        ax2.view_init(40, -40)\n",
    "        ax2.set_title('Predicted test y')\n",
    "        ax2.invert_xaxis()\n",
    "\n",
    "        # ====== Just for Visualizaing with High Resolution ====== #\n",
    "        input_x = torch.Tensor(train_X)\n",
    "        pred_y = model(input_x).detach().numpy() \n",
    "        \n",
    "        ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "        ax3.scatter(train_X[:, 0], train_X[:, 1], pred_y, c=pred_y[:,0], cmap='jet')\n",
    "\n",
    "        ax3.set_xlabel('x1')\n",
    "        ax3.set_ylabel('x2')\n",
    "        ax3.set_zlabel('y')\n",
    "        ax3.set_zlim(-10, 6)\n",
    "        ax3.view_init(40, -40)\n",
    "        ax3.set_title('Predicted train y')\n",
    "        ax3.invert_xaxis()\n",
    "        \n",
    "        plt.show()\n",
    "        print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Report Experiment  \n",
    "학습 과정에서 기록했던 train_loss와 val_loss를 그려봅시다.   \n",
    "Loss가 꾸준히 줄어드는지 확인하고 val_loss가 증가하기 시킨다면 그 이상의 학습은 점점 모델의 성능을 망침을 뜻합니다. \n",
    "그 옆에는 Epoch에 따라서 MAE metric이 얼마나 감소하는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "# ====== Loss Fluctuation ====== #\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(list_epoch, list_train_loss, label='train_loss')\n",
    "ax1.plot(list_epoch, list_val_loss, '--', label='val_loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_ylim(0, 5)\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "ax1.set_title('epoch vs loss')\n",
    "\n",
    "# ====== Metric Fluctuation ====== #\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(list_mae_epoch, list_mae, marker='x', label='mae metric')\n",
    "\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('mae')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "ax2.set_title('epoch vs mae')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "linear regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (comet2)",
   "language": "python",
   "name": "comet2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
